{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: ['af', 'gsw', 'ar', 'an', 'as', 'ast', 'az', 'azb', 'ban', 'bn', 'nan', 'be', 'be-tarask', 'bh', 'bg', 'bs', 'br', 'ca', 'ceb', 'cs', 'cy', 'da', 'de', 'et', 'el', 'es', 'eo', 'eu', 'fa', 'fr', 'gl', 'gu', 'ko', 'ha', 'hy', 'hi', 'hr', 'io', 'id', 'ia', 'is', 'it', 'he', 'ka', 'kk', 'sw', 'ku', 'ky', 'lo', 'la', 'lv', 'lt', 'jbo', 'lmo', 'hu', 'mk', 'ml', 'mr', 'ms', 'mn', 'my', 'fj', 'nl', 'ne', 'ja', 'nqo', 'nb', 'nn', 'or', 'uz', 'pa', 'pnb', 'km', 'nds', 'pl', 'pt', 'kaa', 'ro', 'qu', 'ru', 'sah', 'sat', 'sco', 'sq', 'si', 'en-simple', 'sk', 'sl', 'ckb', 'sr', 'sh', 'fi', 'sv', 'tl', 'ta', 'tt', 'shn', 'te', 'th', 'tg', 'tr', 'bug', 'uk', 'ur', 'ug', 'vi', 'wa', 'lzh', 'war', 'wuu', 'yue', 'zh']\n"
     ]
    }
   ],
   "source": [
    "#Create a function that receives the URL of a Wikipedia page and returns all the alternative languages for the same page.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "def get_wikipedia_languages(url):\n",
    "    r = requests.get(url)\n",
    "    r.raise_for_status()\n",
    "    soup = bs(r.content, 'html.parser')    \n",
    "    lang_links = soup.find_all('a', {'lang': True})\n",
    "  \n",
    "    languages = [link['lang'] for link in lang_links if 'lang' in link.attrs]\n",
    "    return languages\n",
    "\n",
    "    \n",
    "    \n",
    "url = \"https://en.wikipedia.org/wiki/Python_(programming_language)\"\n",
    "languages = get_wikipedia_languages(url)\n",
    "print(\"Languages:\", languages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful! You are now in your Gmail inbox.\n",
      "Extracted 4 unread emails. Report saved to 'emails_report.txt'.\n"
     ]
    }
   ],
   "source": [
    "# Using selenium, authenticate your mailbox and write a file report on all the unread emails you receive.\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By # Used to locate elements on the web page (e.g., by name, ID)\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "def authenticate_mail(driver, email, password):\n",
    "    wait = WebDriverWait(driver, 10) \n",
    "\n",
    "    driver.get(\"https://mail.google.com/\")\n",
    "    email_input = wait.until(EC.presence_of_element_located((By.ID, \"identifierId\")))\n",
    "    email_input.send_keys(email)\n",
    "    email_input.send_keys(Keys.RETURN) # press enter\n",
    "    \n",
    "    password_input = wait.until(EC.presence_of_element_located((By.NAME, \"Passwd\")))\n",
    "    password_input.send_keys(password)\n",
    "    password_input.send_keys(Keys.RETURN)\n",
    "    \n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"div[role='main']\")))  \n",
    "\n",
    "    print(\"Login successful! You are now in your Gmail inbox.\")\n",
    "   \n",
    "    \n",
    "def extract_unread_emails(driver):\n",
    "    try:\n",
    "        time.sleep(5)  # Wait for the page to load fully\n",
    "        unread_emails = driver.find_elements(By.CSS_SELECTOR, 'tr.zA.zE')  \n",
    "        email_data = []\n",
    "\n",
    "        for email in unread_emails:\n",
    "            try:\n",
    "                # Extract sender, subject, and timestamp\n",
    "                sender = email.find_element(By.CSS_SELECTOR, \"[email]\").get_attribute(\"email\")\n",
    "                subject = email.find_element(By.CSS_SELECTOR, \".y6 span\").text\n",
    "                timestamp = email.find_element(By.CSS_SELECTOR, \".xW.xY span\").get_attribute(\"aria-label\")\n",
    "                \n",
    "                email_data.append({\n",
    "                    \"Sender\": sender,\n",
    "                    \"Subject\": subject,\n",
    "                    \"Timestamp\": timestamp,\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error extracting email details: {e}\")\n",
    "\n",
    "        with open(\"emails_report.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            for email in email_data:\n",
    "                file.write(f\"Sender: {email['Sender']}\\n\")\n",
    "                file.write(f\"Subject: {email['Subject']}\\n\")\n",
    "                file.write(f\"Timestamp: {email['Timestamp']}\\n\")\n",
    "                file.write(\"\\n\")\n",
    "\n",
    "        print(f\"Extracted {len(email_data)} unread emails. Report saved to 'emails_report.txt'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    email = \"anom3210oc@gmail.com\"\n",
    "    password = \"gunrvnkl3\"\n",
    "\n",
    "    driver = webdriver.Chrome() \n",
    "\n",
    "    try:\n",
    "        authenticate_mail(driver, email, password)\n",
    "\n",
    "        extract_unread_emails(driver)\n",
    "\n",
    "    finally:\n",
    "        driver.quit()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a simple Wikipedia scraper that collects all the internal links up to a defined depth.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "def get_internal_links(url):\n",
    "    \n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        r.raise_for_status()\n",
    "        soup = bs(r.content, 'html.parser')\n",
    "        anchors = soup.find_all('a', href = True)\n",
    "        \n",
    "        links = []\n",
    "        for href in anchors[\"href\"]:\n",
    "            links.append(href)\n",
    "        \n",
    "        print(links)\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    \n",
    "\n",
    "url = 'https://en.wikipedia.org/wiki/Python_(programming_language)'\n",
    "get_internal_links(url)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
